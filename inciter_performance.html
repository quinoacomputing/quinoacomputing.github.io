<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Inciter performance | Quinoa docs</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,600,600i%7CSource+Code+Pro:400,400i,600" />
  <link rel="stylesheet" href="quinoa.m-dark-noindent+doxygen.compiled.css" />
  <link rel="icon" href="quinoa_sum.png" type="image/png" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="#22272e" />
</head>
<body>
<header><nav id="navigation">
  <div class="m-container">
    <div class="m-row">
      <a href="index.html" id="m-navbar-brand" class="m-col-t-8 m-col-m-none m-left-m">Quinoa <span class="m-thin">docs</span></a>
      <div class="m-col-t-4 m-hide-m m-text-right m-nopadr">
        <a href="#search" class="m-dox-search-icon" title="Search" onclick="return showSearch()"><svg style="height: 0.9rem;" viewBox="0 0 16 16">
          <path d="m6 0c-3.3144 0-6 2.6856-6 6 0 3.3144 2.6856 6 6 6 1.4858 0 2.8463-0.54083 3.8945-1.4355-0.0164 0.33797 0.14734 0.75854 0.5 1.1504l3.2227 3.7891c0.55185 0.6139 1.4517 0.66544 2.002 0.11524 0.55022-0.55022 0.49866-1.4501-0.11524-2.002l-3.7891-3.2246c-0.39184-0.35266-0.81242-0.51469-1.1504-0.5 0.89472-1.0482 1.4355-2.4088 1.4355-3.8945 0-3.3128-2.6856-5.998-6-5.998zm0 1.5625a4.4375 4.4375 0 0 1 4.4375 4.4375 4.4375 4.4375 0 0 1-4.4375 4.4375 4.4375 4.4375 0 0 1-4.4375-4.4375 4.4375 4.4375 0 0 1 4.4375-4.4375z"/>
        </svg></a>
        <a id="m-navbar-show" href="#navigation" title="Show navigation"></a>
        <a id="m-navbar-hide" href="#" title="Hide navigation"></a>
      </div>
      <div id="m-navbar-collapse" class="m-col-t-12 m-show-m m-col-m-none m-right-m">
        <div class="m-row">
          <ol class="m-col-t-6 m-col-m-none">
            <li><a href="why.html">Why</a></li>
            <li>
              <a href="build.html">Build</a>
              <ol>
                <li><a href="git_submodules_subtrees.html">Modules</a></li>
                <li><a href="licenses.html">Libraries</a></li>
                <li><a href="build_system.html">Internals</a></li>
              </ol>
            </li>
            <li>
              <a href="https://quinoacomputing.github.io/index.html#mainpage_tools">Tools</a>
              <ol>
                <li><a href="inciter_main.html">Inciter</a></li>
                <li><a href="unittest_main.html">UnitTest</a></li>
                <li><a href="meshconv_main.html">MeshConv</a></li>
              </ol>
            </li>
            <li><a href="namespaces.html">Namespaces</a></li>
            <li><a href="annotated.html">Classes</a></li>
            <li><a href="files.html">Files</a></li>
          </ol>
          <ol class="m-col-t-6 m-col-m-none" start="7">
            <li>
              <a href="resources.html">Resources</a>
              <ol>
                <li><a href="https://github.com/quinoacomputing/quinoa">GitHub</a></li>
                <li><a href="papers.html">Publications</a></li>
                <li><a href="https://github.com/quinoacomputing/quinoa/blob/master/LICENSE">License</a></li>
                <li><a href="roadmap.html">Roadmap</a></li>
                <li><a href="contributing.html">Contributing</a></li>
                <li><a href="coverage.html">Coverage</a></li>
                <li><a href="https://github.com/quinoacomputing/quinoa/releases">Tarballs</a></li>
                <li><a href="https://bestpractices.coreinfrastructure.org/projects/2120">Practices</a></li>
              </ol>
            </li>
            <li class="m-show-m"><a href="#search" class="m-dox-search-icon" title="Search" onclick="return showSearch()"><svg style="height: 0.9rem;" viewBox="0 0 16 16">
              <path d="m6 0c-3.3144 0-6 2.6856-6 6 0 3.3144 2.6856 6 6 6 1.4858 0 2.8463-0.54083 3.8945-1.4355-0.0164 0.33797 0.14734 0.75854 0.5 1.1504l3.2227 3.7891c0.55185 0.6139 1.4517 0.66544 2.002 0.11524 0.55022-0.55022 0.49866-1.4501-0.11524-2.002l-3.7891-3.2246c-0.39184-0.35266-0.81242-0.51469-1.1504-0.5 0.89472-1.0482 1.4355-2.4088 1.4355-3.8945 0-3.3128-2.6856-5.998-6-5.998zm0 1.5625a4.4375 4.4375 0 0 1 4.4375 4.4375 4.4375 4.4375 0 0 1-4.4375 4.4375 4.4375 4.4375 0 0 1-4.4375-4.4375 4.4375 4.4375 0 0 1 4.4375-4.4375z"/>
            </svg></a></li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</nav></header>
<main><article>
  <div class="m-container m-container-inflatable">
    <div class="m-row">
      <div class="m-col-l-10 m-push-l-1">
        <h1>
          Inciter performance
        </h1>
        <div class="m-block m-default">
          <h3>Contents</h3>
          <ul>
            <li><a href="#inciter_strong_scaling">Strong scaling</a></li>
            <li><a href="#inciter_weak_scaling">Weak scaling</a></li>
            <li><a href="#inciter_overdecomposition">Effects of overdecomposition</a></li>
            <li><a href="#inciter_load_balancing">Load balancing</a></li>
          </ul>
        </div>
<p>This page quantifies different aspects of the computational performance of <a href="inciter_main.html" class="m-dox">Inciter</a>.</p><aside class="m-note m-info"><h4>Note</h4><p>More details on <a href="inciter_main.html" class="m-dox">Inciter</a>&#x27;s performance are given in <a href="papers.html#papers_inciter" class="m-dox">Inciter papers</a>.</p></aside><section id="inciter_strong_scaling"><h2><a href="#inciter_strong_scaling">Strong scaling</a></h2><p>The following figures quantify <a href="https://hpc-wiki.info/hpc/Scaling_tutorial">strong scaling</a> of the different solvers in <a href="inciter_main.html" class="m-dox">Inciter</a>.</p><div class="m-col-m-10 m-center-m"><img class="m-image" src="inciter_scaling_smp.svg" alt="Image" />
 Strong scaling of the time stepping phase (i.e., excluding setup) running <a href="inciter_diagcg.html" class="m-dox">DiagCG</a>, <a href="inciter_alecg.html" class="m-dox">ALECG</a>, and the <a href="inciter_dg.html" class="m-dox">DG</a> single-material Euler solvers using a 794-million-cell mesh without large-file I/O up to about 50 thousand CPUs. The vertical axis here measures wall-clock time. SMP means that Charm++ is running in <a href="https://charm.readthedocs.io/en/latest/charm++/manual.html#machine-model">SMP mode</a>, which is Charm++&#x27;s equivalent of MPI+X where message-based communication is only used on the network, i.a., across compute nodes and not within nodes. One can see that SMP vs. non-SMP mode can make a large difference in absolute performance at large scales. Note that running in SMP mode simply amounts to <a href="build.html#build_smp" class="m-dox">building Charm++ in SMP mode</a> and recompiling Quinoa and Inciter, without any changes to the source code.</div><p>Note that the vertical axis above measures wall-clock time in seconds, thus the lower the value is faster the run was. Since the <code>DG</code> method stores the unknowns at cell centers, and there are a lot more cells than points of the unstructured computational mesh (specifically this mesh had <code>npoin=133,375,577</code> and <code>nelem=794,029,446</code>), the DG method operates on a singificantly larger data (also requiring more memory) and thus solves the same discrete problem with larger fidelity compared to the <code>DiagCG</code> and <code>ALECG</code> (node-centered) schemes. To offer a comparison between the solvers based on the same fidelity, the figure below depicts the same data but normalizing the wall-clock time by the number of degrees of freedom (NDOF). NDOF equals the total number of grid cells for <code>DG</code>, and the total number of nodes for the <code>DiagCG</code> and <code>ALECG</code> solvers.</p><div class="m-col-m-10 m-center-m"><img class="m-image" src="inciter_scaling_ndof.svg" alt="Image" />
 Strong scaling of time stepping running <a href="inciter_diagcg.html" class="m-dox">DiagCG</a>, <a href="inciter_alecg.html" class="m-dox">ALECG</a>, and the <a href="inciter_dg.html" class="m-dox">DG</a> single-material Euler solvers using a 794-million-cell mesh without large-file I/O. The vertical axis here measures wall-clock time normalized by the total number of cells for DG and the total number of nodes for the CG solvers. As before, SMP means that Charm++ is running in SMP mode.</div></section><section id="inciter_weak_scaling"><h2><a href="#inciter_weak_scaling">Weak scaling</a></h2><p>The following figure quantifies the <a href="https://hpc-wiki.info/hpc/Scaling_tutorial">weak scaling scaling</a> of <a href="inciter_diagcg.html" class="m-dox">DiagCG</a>.</p><div class="m-col-m-10 m-center-m"><img class="m-image" src="inciter_weak.svg" alt="Image" />
 Weak scaling of time stepping running <a href="inciter_diagcg.html" class="m-dox">DiagCG</a> without large-file I/O. As before, non-SMP means that Charm++ is running in non-SMP mode. See also <a href="build.html#build_smp" class="m-dox">Build using Charm++&#x27;s SMP mode</a>.</div></section><section id="inciter_overdecomposition"><h2><a href="#inciter_overdecomposition">Effects of overdecomposition</a></h2><p>The figures below demonstrate typical effects of overdecomposition, partitioning the computational domain into <em>more</em> work units than the number of available processors. Overdecomposition helps load balancing, since the runtime system can work with finer-grained work units (compared to a single mesh partition per processing element) that are quicker to migrate and redistribute to homogenize computational load. Another positive effect of overdecomposition is discussed below where smaller work units fit better into CPU caches yielding improved performance.</p><p>The leftmost side of the figures corresponds to the case where the number of work units (<em>chares</em>) equal the number of CPUs &ndash; this is labelled as &quot;classic MPI&quot;, as this is how distributed-memory-parallel codes are traditionally used with the MPI (message passing) paradigm. As the problem is decomposed into more partitions, the chunks become smaller but require more communication as the boundary/domain element ratio increases. Smaller chunks, however, are faster to migrate to other CPUs if needed and fit better into local processor cache. (Note that migration was not enabled for these examples.) As a result the problem can be computed a lot faster, in this case, approximately <strong>50 times(!) faster</strong>. Though finding such sweet spots require experimentation and certainly depends on the problem, problem size, and hardware configuration, the interesting point is that such a large performance gain is possible simply by allowing overdecomposition without the use of multiple software abstractions, e.g., MPI + threading. All of this code is written using a single and high-level parallel computing abstraction: Charm++ <em>without</em> explicit message passing code.</p><div class="m-col-m-10 m-center-m"><img class="m-image" src="inciter_virtualization.png" alt="Image" />
 Total runtime, simply measured by the Unix <em>time</em> utility, including setup and I/O, of integrating the coupled governing equations of mass, momentum, and energy for an ideal gas, using a continuous Galerkin finite element method. The times are normalized and compared to the leftmost (<em>classic MPI</em>) data. As expected, using just a few more partitions per CPU results in a performance degradation as more communication is required. However, further increasing the degree of overdecomposition to about 5 times the number of CPUs yields an excellent speedup of over <strong>10x(!)</strong> due to better cache utilization and overlap of computation and communication.</div><div class="m-col-m-10 m-center-m"><img class="m-image" src="inciter_virtualization_nosetup.png" alt="Image" />
 This figure depicts another manifestation of the effect of overdecomposition: compared to the previous figure, here we only measured the time required to advance the equations without setup and I/O, which is usually the dominant fraction of large scientific computations. The performance gain during time stepping is even larger, reaching almost <strong>50 times(!)</strong> compared to the original run without overdecomposition.</div></section><section id="inciter_load_balancing"><h2><a href="#inciter_load_balancing">Load balancing</a></h2><p>To demonstrate load balancing in parallel, we have run a Sedov problem modified to produce load imbalance. The Sedov problem is an idealized blast originating from a point, leading to a spherically spreading shock wave.</p><p>To induce load imbalance we added some extra computational load to those computational cells whose fluid density exceeds the value of 1.5. This mimics, e.g., combustion, non-trivial material equations of state, etc., and induces realistic load imbalance, characteristic of multi-physics simulations.</p><div class="m-col-m-6 m-left-m"><img class="m-image" src="sedov_load_ts14.svg" alt="Image" /></div><div class="m-col-m-6 m-right-m"><img class="m-image" src="sedov_load_ts50.svg" alt="Image" /></div><p>Spatial distributions of the extra load, corresponding to the fluid density exceeding the value 1.5, during time evolution of the Sedov solution: (left) shortly after the onset of load imbalance, (right) at end of the simulation.</p><p>One can imagine that as the computational domain decomposed into many partitions, residing on multiple compute nodes, the parallel load gets out of balance and work units must all wait for the slowest one.</p><p>Depicted below are simulation times for, running the problem using a 3 million-cell mesh on 10 computes nodes of a distributed-memory cluster with 36 CPUs/node. One can see that the density peak exceeds the value of 1.5 around the 130th time step, inducing load imbalance and this persists until the end of the simulation To balance the load we have used different strategies, available in Charm++, simply by turning on load balancing on the command line.</p><div class="m-col-m-10 m-center-m"><img class="m-image" src="sedov_lb_large.svg" alt="Image" />
 Grind-time during time stepping computing a Sedov problem with extra load in cells whose fluid density exceeds the threshold of 1.5, inducing load imbalance. These simulations were run on a distributed-memory cluster on 10 compute nodes, in SMP mode, using 34 worker threads + 2 communication threads per node, on a 3M-cell mesh. Load balancing was invoked at every 10th time step for all load balancers.</div><p>It is clear from the figure that multiple load balancing strategies can successfully and effectively homogenize the uneven, dynamically generated, a priori unknown computational load. The common requirement for effective load balancing is fine-grained work units, ensured by increasing the degree of overdecomposition (virtualization) from 1 to 100x, which yields 100x the mesh partitions compared to the number of CPUs the problem is running on. As expected, increasing the number of partitions beyond the number of CPUs has a cost due to increased communication cost, c.f., black and red lines. Overall however, load balancing yields over an order of magnitude speedup over the non-balanced case.</p><aside class="m-note m-info"><h4>Note</h4><p>As show above, excellent performance can be obtained using the built-in automatic load balancers of Charm++. <em>It should be emphasized that we wrote no load balancing code: we simply ensure overdecomposition and turn on load balancing; the runtime system measures real-time CPU load and automatically performs object migration to homogenize the load.</em> This is particularly beneficial for applications with <em>a priori</em> unknown or dynamically changing load distribution, characteristic of multiphysics or heterogeneous performance of large data centers. The data in <a href="papers.html#papers_inciter" class="m-dox">Inciter</a>papers&quot; also demonstrates that the cost of load balancing is negligible compared to the savings over the unbalanced problem without load balancing.</p></aside></section>
      </div>
    </div>
  </div>
</article></main>
<div class="m-dox-search" id="search">
  <a href="#!" onclick="return hideSearch()"></a>
  <div class="m-container">
    <div class="m-row">
      <div class="m-col-m-8 m-push-m-2">
        <div class="m-dox-search-header m-text m-small">
          <div><span class="m-label m-default">Tab</span> / <span class="m-label m-default">T</span> to search, <span class="m-label m-default">Esc</span> to close</div>
          <div id="search-symbolcount">&hellip;</div>
        </div>
        <div class="m-dox-search-content">
          <input type="search" id="search-input" placeholder="Loading &hellip;" disabled="disabled" autofocus="autofocus" />
          <noscript class="m-text m-danger m-text-center">Unlike everything else in the docs, the search functionality <em>requires</em> JavaScript. Enable it or <a href="https://google.com/search?q=site:quinoacomputing.github.io+">use an external search engine</a>.</noscript>
          <div id="search-help" class="m-text m-dim m-text-center">
            Search for symbols, directories, files, pages or modules. You can omit any
            prefix from the symbol or file path; adding a <code>:</code> or <code>/</code>
            suffix lists all members of given symbol or directory. Navigate through the
            list using <span class="m-label m-dim">&darr;</span> and
            <span class="m-label m-dim">&uarr;</span>, press
            <span class="m-label m-dim">Enter</span> to go.
          </div>
          <div id="search-notfound" class="m-text m-warning m-text-center">Sorry, nothing was found.<br />Maybe try a full-text <a href="#" id="search-external" data-search-engine="https://google.com/search?q=site:quinoacomputing.github.io+{query}">search with external engine</a>?</div>
          <ul id="search-results"></ul>
        </div>
      </div>
    </div>
  </div>
</div>
<script src="search.js"></script>
<script src="searchdata.js" async="async"></script>
<footer><nav>
  <div class="m-container">
    <div class="m-row">
      <div class="m-col-l-10 m-push-l-1">
        <p>Quinoa docs, part of the <a href="http://quinoacomputing.github.io/">Quinoa project</a>. Copyright © J. Bakosi 2012&ndash;2015, Los Alamos National Security, LLC, 2016&ndash;2018, <a href="https://www.triadns.org/">Triad National Security, LLC,</a> 2019-2021. Generated on Thursday, Dec 01, 2022 based on <a href="https://github.com/quinoacomputing/quinoa/commit/-128-NOTFOUND">-128-NOTFOUND</a> by <a href="http://doxygen.org/">Doxygen</a> and <a href="http://mcss.mosra.cz/">m.css</a>. Contact us via <a href="https://github.com/quinoacomputing/quinoa/issues">GitHub</a>.</p>
      </div>
    </div>
  </div>
</nav></footer>
</body>
</html>