<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Quinoa: Concurrency</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/SVG"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="quinoa.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Quinoa
   </div>
   <div id="projectbrief">Adaptive computational fluid dynamics</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Concurrency </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The strategy chosen for concurrency is probably the most decisive factor affecting performance. It is impossible to pick a single concurrency strategy that is optimal for all the tools and the types of physics (i.e., the different equation-sets implemented), algorithms, and problems. So here, we merely collect the main features of various algorithms from the viewpoint of concurrency. Due to the many factors affecting performance, no ranking is attempted among the approaches.</p>
<h2>1. ALL-TO-ALL-STATS </h2>
<p>This is a back-of-the-envelope estimation for a simple (but naiive) concurrency strategy that is, of course, not used.</p>
<p>Suppose the inhomogeneity strategy is PIC-based (see doc/design/inhomogeneity.txt) and this this concurrency strategy requires a mesh. The idea is to only have to communicate the domain-statistics among distributed compute nodes and nothing else. Concurrency is realized among particles (realizations) of which each compute node generates and advances a different set. Since the full domain, i.e., statistics of all cells that are required for advancing particles, require communication (and that is with all-to-all) at each time-step, this seems like an insane amount of communication and so prohibitive. As it will be shown, it is. The advantage of this strategy is that it does not require communication for anything else, e.g., particle properties, which is assumed to take up the bulk of memory and the bulk of computation cost.</p>
<p>We take a specific algorithmic example, discussed in more detail in <a href="https://doi.org/10.1016/j.jcp.2008.02.024">https://doi.org/10.1016/j.jcp.2008.02.024</a>. This is a continuum-realm PDF method for incompressible flow with elliptic relaxation. In that algorithm only about 5% of the computational time is spent on the two linear solvers (mean pressure and elliptic relaxation) and 92% on advancing the particles. In a plasma-physics-PIC parlance the particle pusher has the overwhelming memory and computational cost, while the cost of field solver is almost negligible.</p>
<p>Two important advantages of this strategy are:</p>
<ol type="1">
<li>Perfect and non-changing load-balance for both particle updates and field-solvers, as the number of particles as well as the mesh on a compute node remain the same throughout time stepping,</li>
<li>Simplicity of the code: since only the mesh-based statistics require communication, there is<ul>
<li>a single large chunk of contiguous array need to be communicated (a single all-to-all per time step),</li>
<li>no need for graph-coloring algorithms,</li>
<li>no need for complex parallel initial I/O and initial load-balancing,</li>
<li>no need for distributed linear solvers (no hard-to-parallelize dot-products, matrix-vector-products, preconditioners; no need for monster linear algebra libraries: no petsc, no trilinos, etc.),</li>
<li>no need for complex low-level compute-node-pair-wise load balancing,</li>
<li>no need for communicating particles and their properties,</li>
<li>no need for dynamic load-balancing (as would be absolutely required by a geometry-based particle-communicating algorithm),</li>
</ul>
</li>
<li>Due to simplicity of code, adaptive mesh refinement (<a class="el" href="namespace_a_m_r.html">AMR</a>) is easier to implement, and separate from the existing field solver.</li>
</ol>
<p>This strategy requires the full Eulerian mesh fit into a single compute node. This is obviously a limitation compared to those distributed concurrency strategies with geometric decomposition, which do not have this limitation. So let's see how much this is a compromise.</p>
<p>In what follows, we only consider 3D unstructured mesh consisting of tetrahedra, but other types of grids should also work with this strategy, and the analysis should not change in outcome. A single tetrahedron consists of 4 vertices, each of which has 3 components. Then the memory cost of a single tetrahedron is </p><pre class="fragment">4 (vertices) x 3 (coordinates) x 8 (double floating point Bytes) = 96 Bytes.
</pre><p>As for all unstructured grids, connectivity information must also be stored, which is an array of 5 integers (1 index + 4 vertex indices) for a tetrahedron. The memory usage of the connectivity is </p><pre class="fragment">5 x 4 (size of int) = 20 Bytes.
</pre><p>The approximate memory cost of a single tetrahedron cell is thus 116 Bytes.</p>
<p>This means that in 32 GB (a common memory size of a single node of a distributed-memory compute cluster), approximately, 296 million tetrahedra fit. Obviously, it's not only the mesh we need in memory, but this is a good approximate upper bound for this back-of-the-envelope memory-estimation for the all-to-all-stats strategy.</p>
<p>Besides the mesh, those statistics that are required for the ongoing simulation, i.e., updating the particles, are also required to be stored in memory. We examine two different algorithms, an existing one and a hypothetical one:</p>
<ol type="1">
<li><p class="startli"><b>Standalone PDF method for incompressible flow with elliptic relaxation,</b> i.e., wall-resolution, see <a href="https://doi.org/10.1016/j.jcp.2008.02.024">https://doi.org/10.1016/j.jcp.2008.02.024</a></p>
<p class="startli">The memory usage of the particle properties per cell: </p><pre class="fragment"> position:                  3 x 8 Bytes
 velocity:                  3 x 8 Bytes
                        total: 48 Bytes
</pre><p class="startli">The memory usage of statistics (defined at grid points or cell-centers) per cell that need to be communicated: </p><pre class="fragment">mean velocity:             3 x 8 Bytes
mean turbulence frequency:     8 Bytes
Reynolds stress tensor:    6 x 8 Bytes
                       total: 80 Bytes
</pre><p class="startli">The memory usage of statistics per cell that do not need to be communicated (but still need to be stored in a compute node's memory, at grid points or cell-centers): </p><pre class="fragment">mean pressure:                 8 Bytes
wiggly-p_{ij}:             9 x 8 Bytes
                       total: 80 Bytes
</pre><p class="startli">We assume 500 particles per cell. Thus the total memory usage of a single cell with particles is </p><pre class="fragment">48 x 500 + 116 + 160 = 24,176 Bytes or approximately 24 KB
</pre><p class="startli">This means that in 32 GB, approximately, 1.4 million cells fit (each with 500 particles).</p>
</li>
<li><p class="startli"><b>Standalone PDF method for compressible flow</b> (hypothetical algorithm with no turbulence model, does not yet exist, we are only guessing what statistics are needed)</p>
<p class="startli">The memory usage of the particle properties per cell: </p><pre class="fragment">position:                  3 x 8 Bytes
density:                       8 Bytes
velocity:                  3 x 8 Bytes
energy:                        8 Bytes
                       total: 64 Bytes
</pre><p class="startli">The memory usage of statistics (defined at grid points or cell-centers) per cell that need to be communicated: </p><pre class="fragment">mean density:                  8 Bytes
mean velocity:             3 x 8 Bytes
mean energy:                   8 Bytes
                       total: 40 Bytes
</pre><p class="startli">The memory usage of statistics per cell that do not need to be communicated (but still need to be stored in a compute node's memory, at grid points or cell-centers), may potentially be zero, if memory storage can be traded by arithmetics, e.g., re-compute the (mean) pressure from the equation of state whenever the pressure is needed.</p>
<p class="startli">We assume 500 particles per cell. Thus the total memory usage of a single cell with particles is </p><pre class="fragment">64 x 500 + 116 + 40 = 32,160 Bytes or approximately 32 KB
</pre><p class="startli">This means that in 32 GB, approximately, 1.0 million cells fit (each with 500 particles).</p>
</li>
</ol>
<p>Obviously, the larger the mesh, the more communication occurs, however, in one single large chunk per time step.</p>
<h3>ALL-TO-ALL-STATS communication cost estimate</h3>
<p>Here is a cost estimate of the necessary all-to-all communication, compared to one-to-one communication required by a geometric concurrency strategy:</p>
<ol type="1">
<li><p class="startli"><b>Geometric decomposition, point-to-point:</b> Using a geometric decomposition of a mesh with N^3 cells requires only point-to-point (p2p) communications but of particles and their properties. If the computational domain is a cube with sides of N cells, each cell has n particles with r physical properties, and we assume explicit time stepping with approximately unit CFL (i.e., approximately all particles leave their cell on a compute node boundary and therefore must be communicated), the cost of communication is</p>
<p class="formulaDsp">
\[ C(\textrm{geo+p2p}) = N^2 * n * r * \log_2(P) \]
</p>
<p class="startli">Here <em>P</em> is the number of compute nodes (among which communication is required). The <em>N^2</em> comes from the geometric nature of subdivision of the <em>N^3</em> domain: every slice will add a communication cost of a <em>N^2-cost</em> surface (along which communication must take place). The geometric decomposition also results in doubling the number of compute nodes with each division, hence the <em>log2(P)-dependence</em>. Typical values of <em>n</em> are in the range of 100-500, while <em>r</em> could be as few as 6 (incompressible flow: 3 position + 3 velocity components), and as many as 100s (single-material flow</p><ul>
<li>chemistry with approximately 90 species).</li>
</ul>
</li>
</ol>
<div class="image">
<object type="image/svg+xml" data="https://quinoacomputing.github.io/all2allcost.svg" align="right" width="50%" background="transparent"></object>
</div>
<ol type="1">
<li><p class="startli"><b>Particle-array-slices, all-to-all:</b> Using an all-to-all decomposition (i.e., only on the particle properties are subdivided), yields a communication cost of</p>
<p class="formulaDsp">
\[ C(par+a2a) = N^3 * r * (P-1) \]
</p>
<p class="startli">since the particles are not, but the statistics (at least the means) must still be communicated. (However, higher order-statistics might still be required for advanced mixing models, further increasing communication costs.)</p>
<p class="startli">Plotting the above two cost-functions (using N=100, n=100, r=100), </p><pre class="fragment">gnuplot&gt; plot [1:1e+7] 1.0e+8*log(x)/log(2), 1.0e8*(x-1)
</pre><p class="startli">reveals that, not surprisingly, <em>the all-to-all strategy is prohibitively expensive, for practical numbers of compute nodes. Additionally, the non-geometric strategy has the major limitation that the full mesh must fit into every compute node's memory. This major limitation combined with the prohibitive communication cost renders the all-to-all strategy not worthwhile.</em></p>
</li>
</ol>
<hr/>
 <div><small> <em>Page last updated:</em> Thu 06 Apr 2017 10:44:23 AM MDT <em>Copyright 2012-2015, J. Bakosi, 2016-2018, Los Alamos National Security, LLC.</em> </small></div> </div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Fri Mar 30 2018 21:46:19 for Quinoa by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
